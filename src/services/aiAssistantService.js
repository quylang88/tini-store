/**
 * aiAssistantService.js
 * "Bá»™ nÃ£o" xá»­ lÃ½ logic cho Trá»£ lÃ½ Quáº£n lÃ½ Tiny Shop.
 */

import { getModeConfig, PROVIDERS } from "./ai/config";
import { callGeminiAPI, callGroqAPI, searchWeb } from "./ai/providers";
import { buildDynamicSystemPrompt, buildSummarizePrompt } from "./ai/prompts";
import {
  getCurrentLocation,
  getAddressFromCoordinates,
} from "./ai/locationUtils";
import { createResponse } from "./ai/chatHelpers";
import { INVENTORY_TOOLS } from "./ai/toolsDefinitions";
import { checkDuplicateQuery } from "./ai/textAnalysisUtils";
import { detectIntent } from "./ai/intentService";

// --- Cáº¤U HÃŒNH MEMORY ---
const SLIDING_WINDOW_SIZE = 6;

// --- Xá»¬ LÃ CHÃNH ---

export const processQuery = async (
  query,
  context,
  modeKey = "standard",
  history = [],
  currentSummary = "",
  onStatusUpdate = () => {},
  explicitIntent = null,
) => {
  if (!navigator.onLine) {
    return createResponse("text", "Máº¥t máº¡ng rá»“i máº¹ Trang Æ¡i ðŸ¥º");
  }

  const modeConfig = getModeConfig(modeKey);

  // 0. Parallel Execution: Intent & Location
  // Cháº¡y song song Ä‘á»ƒ tá»‘i Æ°u thá»i gian pháº£n há»“i (giáº£m 50% wait time náº¿u balanced)

  // Task A: Intent Detection
  const intentPromise = (async () => {
    if (explicitIntent) return explicitIntent;
    try {
      onStatusUpdate("Misa Ä‘ang suy nghÄ©...");
      return await detectIntent(query);
    } catch (err) {
      console.warn("Intent check failed, fallback to CHAT:", err);
      return "CHAT";
    }
  })();

  // Task B: Location Context
  const locationPromise = (async () => {
    try {
      const coords = await getCurrentLocation();
      if (!coords) return "ChÆ°a rÃµ";

      let info = `${coords}`;
      const locName = await getAddressFromCoordinates(coords);
      if (locName) info = `${locName} (${coords})`;
      return info;
    } catch (err) {
      console.warn("Location check failed:", err);
      return "ChÆ°a rÃµ";
    }
  })();

  const [intent, fullLocationInfo] = await Promise.all([
    intentPromise,
    locationPromise,
  ]);

  // 2. Logic TÃ¬m kiáº¿m (Simplified based on Intent)
  const shouldSearch =
    intent === "SEARCH" || (modeKey === "deep" && query.length > 3);

  let searchResults = null;

  if (shouldSearch) {
    onStatusUpdate("Misa Ä‘ang Ä‘i soi giÃ¡ thá»‹ trÆ°á»ng...");
    const lowerQuery = query.toLowerCase();
    let searchQuery = query;
    if (
      (lowerQuery.includes("giÃ¡") || lowerQuery.includes("nháº­p")) &&
      !lowerQuery.includes("nháº­t")
    ) {
      searchQuery += " price Japan Rakuten Amazon JP";
    }

    try {
      searchResults = await searchWeb(
        searchQuery,
        fullLocationInfo,
        modeConfig.search_depth,
        modeConfig.max_results,
      );
    } catch (err) {
      console.warn("Search failed:", err);
    }
    onStatusUpdate(null);
  }

  // 3. Xá»­ lÃ½ Lá»‹ch sá»­
  const userMessages = history.filter(
    (msg) => msg.sender === "user" || msg.role === "user",
  );
  let isDuplicate = false;
  if (userMessages.length >= 2) {
    isDuplicate = checkDuplicateQuery(
      query,
      userMessages[userMessages.length - 2].content,
    );
  }

  const cleanHistory = history
    .filter(
      (msg) =>
        (msg.sender === "user" || msg.sender === "assistant") &&
        msg.type !== "error",
    )
    .map((msg) => ({
      role: msg.sender === "user" ? "user" : "assistant",
      content: msg.content,
    }));

  const recentHistory = cleanHistory.slice(-SLIDING_WINDOW_SIZE);

  // 4. Build Dynamic Prompt based on Intent
  const systemInstruction = buildDynamicSystemPrompt(
    intent,
    { ...context, location: fullLocationInfo },
    searchResults,
    currentSummary,
    isDuplicate,
  );

  // 5. Gá»i AI vá»›i Tools
  try {
    // Chá»‰ báº­t Tool khi Intent lÃ  IMPORT hoáº·c EXPORT Ä‘á»ƒ trÃ¡nh áº£o giÃ¡c (hallucination)
    // khi AI cá»‘ gá»i tool trong lÃºc Ä‘ang tÆ° váº¥n (SEARCH) hoáº·c tÃ¡n gáº«u (CHAT).
    const availableTools = ["IMPORT", "EXPORT"].includes(intent)
      ? INVENTORY_TOOLS
      : null;

    const result = await processQueryWithFailover(
      modeConfig.model,
      recentHistory,
      systemInstruction,
      modeConfig.temperature,
      availableTools,
    );

    // Ká»ŠCH Báº¢N A: AI muá»‘n dÃ¹ng Tool
    if (result.tool_calls && result.tool_calls.length > 0) {
      const toolCall = result.tool_calls[0];
      try {
        const args = JSON.parse(toolCall.function.arguments);
        return createResponse(
          "tool_request",
          result.content || "Äá»£i Misa má»™t xÃ­u nha...",
          {
            toolCallId: toolCall.id,
            functionName: toolCall.function.name,
            functionArgs: args,
            // KhÃ´ng cáº§n rawToolCallMessage ná»¯a vÃ¬ Ä‘Ã£ parse xong
          },
        );
      } catch (e) {
        console.error("Lá»—i parse arguments tá»« AI:", e);
        return createResponse(
          "text",
          "Misa Ä‘á»‹nh lÃ m gÃ¬ Ä‘Ã³ mÃ  quÃªn máº¥t cÃ¡ch lÃ m rá»“i huhu.",
        );
      }
    }

    // Ká»ŠCH Báº¢N B: Chat thÆ°á»ng
    return createResponse("text", result.content);
  } catch (error) {
    console.error("AI Service Error:", error);
    return createResponse("text", `Lá»—i rá»“i: ${error.message}`);
  }
};

/**
 * HÃ m há»— trá»£ xá»­ lÃ½ káº¿t quáº£ sau khi cháº¡y Tool (Turn 2)
 * Gá»i láº¡i AI vá»›i káº¿t quáº£ thá»±c thi Ä‘á»ƒ AI chÃ©m giÃ³ tiáº¿p.
 */
export const processToolResult = async (
  originalQuery,
  context,
  history,
  toolCallData, // { toolCallId, functionName, functionArgs }
  toolOutputString,
  modeKey = "standard",
) => {
  const modeConfig = getModeConfig(modeKey);

  // Khi xá»­ lÃ½ káº¿t quáº£ tool, thÆ°á»ng lÃ  Ä‘Ã£ xong viá»‡c, quay vá» CHAT hoáº·c giá»¯ context cÆ¡ báº£n.
  // Ta dÃ¹ng intent='CHAT' Ä‘á»ƒ load Common Prompt (cÃ³ product list má»›i nháº¥t) mÃ  khÃ´ng cáº§n rules phá»©c táº¡p.
  const systemInstruction = buildDynamicSystemPrompt(
    "CHAT",
    context,
    null,
    "",
    false,
  );

  // XÃ¢y dá»±ng history Ä‘áº·c biá»‡t cho turn nÃ y theo chuáº©n OpenAI/Groq:
  // 1. History cÅ©
  // 2. User Query (cÃ¢u lá»‡nh dáº«n Ä‘áº¿n viá»‡c gá»i tool)
  // 3. Assistant Message (chá»©a tool_calls)
  // 4. Tool Message (chá»©a káº¿t quáº£ tool)

  const cleanHistory = history.map((m) => ({
    role: m.sender === "user" ? "user" : "assistant",
    content: m.content,
  }));

  const conversation = [
    ...cleanHistory,
    { role: "user", content: originalQuery },
    {
      role: "assistant",
      content: null, // Message gá»i tool thÆ°á»ng khÃ´ng cÃ³ content
      tool_calls: [
        {
          id: toolCallData.toolCallId,
          type: "function",
          function: {
            name: toolCallData.functionName,
            arguments: JSON.stringify(toolCallData.functionArgs),
          },
        },
      ],
    },
    {
      role: "tool",
      tool_call_id: toolCallData.toolCallId,
      content: toolOutputString,
    },
  ];

  try {
    // Sá»­ dá»¥ng chung luá»“ng failover, Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n
    // CÃ¡c provider Ä‘Ã£ Ä‘Æ°á»£c update Ä‘á»ƒ xá»­ lÃ½ message cÃ³ role='tool' vÃ  tool_calls
    const result = await processQueryWithFailover(
      modeConfig.model,
      conversation,
      systemInstruction,
      modeConfig.temperature,
      INVENTORY_TOOLS,
    );

    return createResponse("text", result.content);
  } catch (e) {
    console.error("Tool Result processing failed", e);
    // Fallback náº¿u AI cháº¿t
    return createResponse(
      "text",
      `Xong rá»“i nha! (Chi tiáº¿t: ${toolOutputString})`,
    );
  }
};

export const summarizeChatHistory = async (
  currentSummary,
  messagesToSummarize,
) => {
  if (!messagesToSummarize || messagesToSummarize.length === 0)
    return currentSummary;
  const fastModel = [
    {
      provider: PROVIDERS.GROQ,
      model: import.meta.env.VITE_GROQ_MODEL_INSTANT,
    },
  ];
  const cleanMessages = messagesToSummarize.map((m) => ({
    role: m.sender,
    content: m.content,
  }));
  const prompt = buildSummarizePrompt(currentSummary, cleanMessages);
  try {
    return await processQueryWithFailover(fastModel, [], prompt, 0.3).then(
      (res) => res.content,
    );
  } catch {
    return currentSummary;
  }
};

const processQueryWithFailover = async (
  candidates,
  chatHistory,
  systemInstruction,
  temperature,
  tools = null,
) => {
  let lastError = null;
  for (const candidate of candidates) {
    const { provider, model } = candidate;
    if (!model) continue;
    try {
      if (provider === PROVIDERS.GEMINI) {
        return await callGeminiAPI(
          model,
          chatHistory,
          systemInstruction,
          temperature,
        );
      } else if (provider === PROVIDERS.GROQ) {
        return await callGroqAPI(
          model,
          chatHistory,
          systemInstruction,
          temperature,
          tools,
        );
      }
    } catch (error) {
      console.error(`Lá»—i ${provider}:`, error);
      lastError = error;
      continue;
    }
  }
  throw lastError || new Error("All models failed.");
};
